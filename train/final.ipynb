{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import pandas as pd\n",
    "from pyvi import ViTokenizer\n",
    "import re\n",
    "import string\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VN_CHARS_LOWER = u'ạảãàáâậầấẩẫăắằặẳẵóòọõỏôộổỗồốơờớợởỡéèẻẹẽêếềệểễúùụủũưựữửừứíìịỉĩýỳỷỵỹđð'\n",
    "VN_CHARS_UPPER = u'ẠẢÃÀÁÂẬẦẤẨẪĂẮẰẶẲẴÓÒỌÕỎÔỘỔỖỒỐƠỜỚỢỞỠÉÈẺẸẼÊẾỀỆỂỄÚÙỤỦŨƯỰỮỬỪỨÍÌỊỈĨÝỲỶỴỸÐĐ'\n",
    "VN_CHARS = VN_CHARS_LOWER + VN_CHARS_UPPER\n",
    "def no_marks(s):\n",
    "    __INTAB = [ch for ch in VN_CHARS]\n",
    "    __OUTTAB = \"a\"*17 + \"o\"*17 + \"e\"*11 + \"u\"*11 + \"i\"*5 + \"y\"*5 + \"d\"*2\n",
    "    __OUTTAB += \"A\"*17 + \"O\"*17 + \"E\"*11 + \"U\"*11 + \"I\"*5 + \"Y\"*5 + \"D\"*2\n",
    "    __r = re.compile(\"|\".join(__INTAB))\n",
    "    __replaces_dict = dict(zip(__INTAB, __OUTTAB))\n",
    "    result = __r.sub(lambda m: __replaces_dict[m.group(0)], s)\n",
    "    return result\n",
    "\n",
    "def normalize_text(text):\n",
    "\n",
    "    #Remove các ký tự kéo dài: vd: đẹppppppp\n",
    "    text = re.sub(r'([A-Z])\\1+', lambda m: m.group(1).upper(), text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Chuyển thành chữ thường\n",
    "    text = text.lower()\n",
    "\n",
    "    #Chuẩn hóa tiếng Việt, xử lý emoj, chuẩn hóa tiếng Anh, thuật ngữ\n",
    "    replace_list = {\n",
    "        'òa': 'oà', 'óa': 'oá', 'ỏa': 'oả', 'õa': 'oã', 'ọa': 'oạ', 'òe': 'oè', 'óe': 'oé','ỏe': 'oẻ',\n",
    "        'õe': 'oẽ', 'ọe': 'oẹ', 'ùy': 'uỳ', 'úy': 'uý', 'ủy': 'uỷ', 'ũy': 'uỹ','ụy': 'uỵ', 'uả': 'ủa',\n",
    "        'ả': 'ả', 'ố': 'ố', 'u´': 'ố','ỗ': 'ỗ', 'ồ': 'ồ', 'ổ': 'ổ', 'ấ': 'ấ', 'ẫ': 'ẫ', 'ẩ': 'ẩ',\n",
    "        'ầ': 'ầ', 'ỏ': 'ỏ', 'ề': 'ề','ễ': 'ễ', 'ắ': 'ắ', 'ủ': 'ủ', 'ế': 'ế', 'ở': 'ở', 'ỉ': 'ỉ',\n",
    "        'ẻ': 'ẻ', 'àk': u' à ','aˋ': 'à', 'iˋ': 'ì', 'ă´': 'ắ','ử': 'ử', 'e˜': 'ẽ', 'y˜': 'ỹ', 'a´': 'á',\n",
    "        \n",
    "        #Chuẩn hóa 1 số sentiment words/English words\n",
    "        ':))': '  positive ', ':)': ' positive ', 'ô kêi': ' ok ', 'okie': ' ok ', ' o kê ': ' ok ',\n",
    "        'okey': ' ok ', 'ôkê': ' ok ', 'oki': ' ok ', ' oke ':  ' ok ',' okay':' ok ','okê':' ok ',\n",
    "        ' tks ': u' cám ơn ', 'thks': u' cám ơn ', 'thanks': u' cám ơn ', 'ths': u' cám ơn ', 'thank': u' cám ơn ',\n",
    "        '⭐': 'star ', '*': 'star ', '🌟': 'star ', '🎉': u' 5star ',\n",
    "        'kg ': u' không ','not': u' không ', u' kg ': u' không ', '\"k ': u' không ',' kh ':u' không ','kô':u' không ','hok':u' không ',' kp ': u' không phải ',u' kô ': u' không ', '\"ko ': u' không ', u' ko ': u' không ', u' k ': u' không ', 'khong': u' không ', u' hok ': u' không ',\n",
    "        'he he': ' 5star ','hehe': ' 5star ','hihi': ' 5star ', 'haha': ' 5star ', 'hjhj': ' 5star ',\n",
    "        ' lol ': ' 1star ',' cc ': ' 1star ','cute': u' dễ thương ','huhu': ' 1star ', ' vs ': u' với ', 'wa': ' quá ', 'wá': u' quá', 'j': u' gì ', '“': ' ',\n",
    "        ' sz ': u' cỡ ', 'size': u' cỡ ', u' đx ': u' được ', 'dk': u' được ', 'dc': u' được ', 'đk': u' được ',\n",
    "        'đc': u' được ','authentic': u' chuẩn chính hãng ',u' aut ': u' chuẩn chính hãng ', u' auth ': u' chuẩn chính hãng ', 'thick': u' positive ', 'store': u' cửa hàng ',\n",
    "        'shop': u' cửa hàng ', 'sp': u' sản phẩm ', 'gud': u' tốt ','god': u' tốt ','wel done':' tốt ', 'good': u' tốt ', 'gút': u' tốt ','great': u' tốt ',\n",
    "        'sấu': u' xấu ','gut': u' tốt ', u' tot ': u' tốt ', u' nice ': u' tốt ', 'perfect': 'rất tốt', 'bt': u' bình thường ',\n",
    "        'time': u' thời gian ', 'qá': u' quá ', u' ship ': u' giao hàng ', u' m ': u' mình ', u' mik ': u' mình ',\n",
    "        'ể': 'ể', 'product': 'sản phẩm', 'quality': 'chất lượng','chat':' chất ', 'excelent': 'hoàn hảo', 'bad': 'tệ','fresh': ' tươi ','sad': ' tệ ',\n",
    "        'date': u' hạn sử dụng ', 'hsd': u' hạn sử dụng ','quickly': u' nhanh ', 'quick': u' nhanh ','fast': u' nhanh ','delivery': u' giao hàng ',u' síp ': u' giao hàng ',\n",
    "        'beautiful': u' đẹp tuyệt vời ', u' tl ': u' trả lời ', u' r ': u' rồi ', u' shopE ': u' cửa hàng ',u' order ': u' đặt hàng ',\n",
    "        'chất lg': u' chất lượng ',u' sd ': u' sử dụng ',u' dt ': u' điện thoại ',u' nt ': u' nhắn tin ',u' tl ': u' trả lời ',u' sài ': u' xài ',u'bjo':u' bao giờ ',\n",
    "        'thik': u' thích ',u' sop ': u' cửa hàng ', ' fb ': ' facebook ', ' face ': ' facebook ', ' very ': u' rất ',u'quả ng ':u' quảng  ',\n",
    "        'dep': u' đẹp ',u' xau ': u' xấu ','delicious': u' ngon ', u'hàg': u' hàng ', u'qủa': u' quả ',\n",
    "        'iu': u' yêu ','fake': u' giả mạo ', 'trl': 'trả lời', '><': u' 5star ',\n",
    "        ' por ': u' tệ ',' poor ': u' tệ ', 'ib':u' nhắn tin ', 'rep':u' trả lời ',u'fback':' feedback ','fedback':' feedback ',\n",
    "        # quy các icon về 2 loại 1 sao và 5 sao:\n",
    "        \"👹\": \"1star\", \"👻\": \"5star\", \"💃\": \"5star\",'🤙': ' 5star ', '👍': ' 5star ',\n",
    "        \"💄\": \"5star\", \"💎\": \"5star\", \"💩\": \"5star\",\"😕\": \"1star\", \"😱\": \"1star\", \"😸\": \"5star\",\n",
    "        \"😾\": \"1star\", \"🚫\": \"1star\",  \"🤬\": \"1star\",\"🧚\": \"5star\", \"🧡\": \"5star\",'🐶':' 5star ',\n",
    "        '👎': ' 1star ', '😣': ' 1star ','✨': ' 5star ', '❣': ' 5star ','☀': ' 5star ',\n",
    "        '♥': ' 5star ', '🤩': ' 5star ', 'like': ' 5star ', '💌': ' 5star ',\n",
    "        '🤣': ' 5star ', '🖤': ' 5star ', '🤤': ' 5star ', ':(': ' 1star ', '😢': ' 1star ',\n",
    "        '❤': ' 5star ', '😍': ' 5star ', '😘': ' 5star ', '😪': ' 1star ', '😊': ' 5star ',\n",
    "        '?': ' ? ', '😁': ' 5star ', '💖': ' 5star ', '😟': ' 1star ', '😭': ' 1star ',\n",
    "        '💯': ' 5star ', '💗': ' 5star ', '♡': ' 5star ', '💜': ' 5star ', '🤗': ' 5star ',\n",
    "        '^^': ' 5star ', '😨': ' 1star ', '☺': ' 5star ', '💋': ' 5star ', '👌': ' 5star ',\n",
    "        '😖': ' 1star ', '😀': ' 5star ', ':((': ' 1star ', '😡': ' 1star ', '😠': ' 1star ',\n",
    "        '😒': ' 1star ', '🙂': ' 5star ', '😏': ' 1star ', '😝': ' 5star ', '😄': ' 5star ',\n",
    "        '😙': ' 5star ', '😤': ' 1star ', '😎': ' 5star ', '😆': ' 5star ', '💚': ' 5star ',\n",
    "        '✌': ' 5star ', '💕': ' 5star ', '😞': ' 1star ', '😓': ' 1star ', '️🆗️': ' 5star ',\n",
    "        '😉': ' 5star ', '😂': ' 5star ', ':v': '  5star ', '=))': '  5star ', '😋': ' 5star ',\n",
    "        '💓': ' 5star ', '😐': ' 1star ', ':3': ' 5star ', '😫': ' 1star ', '😥': ' 1star ',\n",
    "        '😃': ' 5star ', '😬': ' 😬 ', '😌': ' 😌 ', '💛': ' 5star ', '🤝': ' 5star ', '🎈': ' 5star ',\n",
    "        '😗': ' 5star ', '🤔': ' 1star ', '😑': ' 1star ', '🔥': ' 1star ', '🙏': ' 1star ',\n",
    "        '🆗': ' 5star ', '😻': ' 5star ', '💙': ' 5star ', '💟': ' 5star ',\n",
    "        '😚': ' 5star ', '❌': ' 1star ', '👏': ' 5star ', ';)': ' 5star ', '<3': ' 5star ',\n",
    "        '🌝': ' 5star ',  '🌷': ' 5star ', '🌸': ' 5star ', '🌺': ' 5star ',\n",
    "        '🌼': ' 5star ', '🍓': ' 5star ', '🐅': ' 5star ', '🐾': ' 5star ', '👉': ' 5star ',\n",
    "        '💐': ' 5star ', '💞': ' 5star ', '💥': ' 5star ', '💪': ' 5star ',\n",
    "        '💰': ' 5star ',  '😇': ' 5star ', '😛': ' 5star ', '😜': ' 5star ',\n",
    "        '🙃': ' 5star ', '🤑': ' 5star ', '🤪': ' 5star ','☹': ' 1star ',  '💀': ' 1star ',\n",
    "        '😔': ' 1star ', '😧': ' 1star ', '😩': ' 1star ', '😰': ' 1star ', '😳': ' 1star ',\n",
    "        '😵': ' 1star ', '😶': ' 1star ', '🙁': ' 1star ',\n",
    "        #\n",
    "        'năm sao': ' 5star ','5 sao': ' 5star ', '4 sao': ' 4star ','bốn sao': ' 4star ','3 sao': ' 3star ',\n",
    "        'ba sao': ' 3star ', '2 sao': ' 2star ', 'hai sao': ' 2star ','1 sao':' 1star ','một sao':' 1star ',\n",
    "        }\n",
    "\n",
    "    for k, v in replace_list.items():\n",
    "        text = text.replace(k, v)\n",
    "\n",
    "    # chuyen punctuation thành space\n",
    "    translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "    text = text.translate(translator)\n",
    "\n",
    "    text = ViTokenizer.tokenize(text)\n",
    "    texts = text.split()\n",
    "    len_text = len(texts)\n",
    "\n",
    "    texts = [t.replace('_', ' ') for t in texts]\n",
    "    \n",
    "\n",
    "    text = u' '.join(texts)\n",
    "\n",
    "    #remove nốt những ký tự thừa thãi\n",
    "    text = text.replace(u'\"', u' ')\n",
    "    text = text.replace(u'️', u'')\n",
    "    text = text.replace('🏻','')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSource(object):\n",
    "  def load_data(self, filename):\n",
    "    star= []\n",
    "    review = []\n",
    "\n",
    "    with open(filename, 'r') as fp:\n",
    "        line = fp.readline()\n",
    "        while line:\n",
    "            star.append(line[0])\n",
    "            review.append(line[2:])\n",
    "            line = fp.readline()\n",
    "    \n",
    "    return star, review\n",
    "    \n",
    "  def load_test_data(self, filename):\n",
    "    star= []\n",
    "    review = []\n",
    "\n",
    "    with open(filename, 'r') as fp:\n",
    "        line = fp.readline()\n",
    "        while line:\n",
    "            star.append(line[1])\n",
    "            review.append(line[3:-1])\n",
    "            line = fp.readline()\n",
    "    \n",
    "    return star, review\n",
    "  def load_data_2(self, filename):\n",
    "    star= []\n",
    "    review = []\n",
    "    with open(filename, 'r') as csv_file:\n",
    "      i = 0\n",
    "      for row in csv_file:\n",
    "          row = row.split('<fff>')\n",
    "          star.append(row[0])\n",
    "          review.append(row[1])\n",
    "    return star, review\n",
    "  def transform_to_dataset(self, x_set, y_set):\n",
    "    X, y = [], []\n",
    "    for document, topic in zip(list(x_set), list(y_set)):\n",
    "        document = normalize_text(document)\n",
    "        X.append(document.strip())\n",
    "        y.append(topic)\n",
    "    return X, y\n",
    "  def transform_to_dataset_with_augmentation(self, x_set, y_set):\n",
    "    X, y = [], []\n",
    "    for document, topic in zip(list(x_set), list(y_set)):\n",
    "        document = normalize_text(document)\n",
    "        X.append(document.strip())\n",
    "        y.append(topic)\n",
    "        # Augmentation bằng cách remove dấu tiếng Việt\n",
    "        X.append(no_marks(document))\n",
    "        y.append(topic)\n",
    "    return X, y\n",
    "  def get_predicted_result(self, star, review, filename):\n",
    "    f = open(filename, \"a\")\n",
    "    for i in range(len(star)):\n",
    "      line = str(star[i]) + \" \" + review[i] + \"\\n\"\n",
    "      f.write(line)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DataSource()\n",
    "star, review = ds.load_data('train.txt')\n",
    "star2, review2 = ds.load_data_2('bag_text.txt')\n",
    "star_test, review_test = ds.load_test_data('test.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  X_train, X_test, y_train, y_test = train_test_split(review, star, test_size=0.6, random_state=42)\n",
    "\n",
    "X_train = review\n",
    "y_train = star\n",
    "X_train = X_train * 50 +  review2 * 50 \n",
    "y_train = y_train * 50 +  star2 * 50\n",
    "X_train, y_train = ds.transform_to_dataset_with_augmentation(X_train,y_train)\n",
    "X_test, y_test = ds.transform_to_dataset(review_test, star_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_ws = (u'rằng',u'thì',u'là')\n",
    "#Try some models\n",
    "classifiers = [\n",
    "            MultinomialNB(),\n",
    "            # DecisionTreeClassifier(),\n",
    "            # LogisticRegression(),\n",
    "            SGDClassifier(),\n",
    "            LinearSVC(fit_intercept = True,multi_class='crammer_singer', C=1),\n",
    "            # RandomForestClassifier(),\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'5': 450, '4': 99, '3': 31, '1': 19, '2': 1})\n",
      "Counter({'5': 400, '4': 114, '3': 61, '1': 24, '2': 1})\n",
      "Counter({'5': 412, '4': 108, '3': 55, '1': 23, '2': 2})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hatake/.local/lib/python3.6/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,4))\n",
    "vectors = vectorizer.fit_transform(X_train)\n",
    "vectors1 = vectorizer.transform(X_test)\n",
    "i = 1\n",
    "for classifier in classifiers:\n",
    "    filename = \"result_\" + str(i) + \".txt\"\n",
    "#     steps = []\n",
    "#     steps.append(('CountVectorizer', CountVectorizer(ngram_range=(1,3),stop_words=stop_ws)))\n",
    "#     steps.append(('tfidf', TfidfTransformer(use_idf=False, sublinear_tf = True,norm='l2',smooth_idf=True)))\n",
    "#     steps.append(('classifier', classifier))\n",
    "#     clf = Pipeline(steps)\n",
    "#     clf.fit(X_train, y_train)\n",
    "    \n",
    "    classifier.fit(vectors, y_train)\n",
    "    y_pred = classifier.predict(vectors1)\n",
    "    print(Counter(y_pred))\n",
    "    ds.get_predicted_result(y_pred, review_test, filename)\n",
    "    i = i+1\n",
    "#     score = accuracy_score(y_test, y_pred)\n",
    "#     print(classifier)\n",
    "#     print(score)\n",
    "#     print(confusion_matrix(y_test, y_pred))\n",
    "#     print(cross_score = cross_val_score(classifier, X_train,y_train, cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
