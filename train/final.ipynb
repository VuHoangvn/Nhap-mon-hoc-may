{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import pandas as pd\n",
    "from pyvi import ViTokenizer\n",
    "import re\n",
    "import string\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VN_CHARS_LOWER = u'·∫°·∫£√£√†√°√¢·∫≠·∫ß·∫•·∫©·∫´ƒÉ·∫Ø·∫±·∫∑·∫≥·∫µ√≥√≤·ªç√µ·ªè√¥·ªô·ªï·ªó·ªì·ªë∆°·ªù·ªõ·ª£·ªü·ª°√©√®·∫ª·∫π·∫Ω√™·∫ø·ªÅ·ªá·ªÉ·ªÖ√∫√π·ª•·ªß≈©∆∞·ª±·ªØ·ª≠·ª´·ª©√≠√¨·ªã·ªâƒ©√Ω·ª≥·ª∑·ªµ·ªπƒë√∞'\n",
    "VN_CHARS_UPPER = u'·∫†·∫¢√É√Ä√Å√Ç·∫¨·∫¶·∫§·∫®·∫™ƒÇ·∫Æ·∫∞·∫∂·∫≤·∫¥√ì√í·ªå√ï·ªé√î·ªò·ªî·ªñ·ªí·ªê∆†·ªú·ªö·ª¢·ªû·ª†√â√à·∫∫·∫∏·∫º√ä·∫æ·ªÄ·ªÜ·ªÇ·ªÑ√ö√ô·ª§·ª¶≈®∆Ø·ª∞·ªÆ·ª¨·ª™·ª®√ç√å·ªä·ªàƒ®√ù·ª≤·ª∂·ª¥·ª∏√êƒê'\n",
    "VN_CHARS = VN_CHARS_LOWER + VN_CHARS_UPPER\n",
    "def no_marks(s):\n",
    "    __INTAB = [ch for ch in VN_CHARS]\n",
    "    __OUTTAB = \"a\"*17 + \"o\"*17 + \"e\"*11 + \"u\"*11 + \"i\"*5 + \"y\"*5 + \"d\"*2\n",
    "    __OUTTAB += \"A\"*17 + \"O\"*17 + \"E\"*11 + \"U\"*11 + \"I\"*5 + \"Y\"*5 + \"D\"*2\n",
    "    __r = re.compile(\"|\".join(__INTAB))\n",
    "    __replaces_dict = dict(zip(__INTAB, __OUTTAB))\n",
    "    result = __r.sub(lambda m: __replaces_dict[m.group(0)], s)\n",
    "    return result\n",
    "\n",
    "def normalize_text(text):\n",
    "\n",
    "    #Remove c√°c k√Ω t·ª± k√©o d√†i: vd: ƒë·∫πppppppp\n",
    "    text = re.sub(r'([A-Z])\\1+', lambda m: m.group(1).upper(), text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Chuy·ªÉn th√†nh ch·ªØ th∆∞·ªùng\n",
    "    text = text.lower()\n",
    "\n",
    "    #Chu·∫©n h√≥a ti·∫øng Vi·ªát, x·ª≠ l√Ω emoj, chu·∫©n h√≥a ti·∫øng Anh, thu·∫≠t ng·ªØ\n",
    "    replace_list = {\n",
    "        '√≤a': 'o√†', '√≥a': 'o√°', '·ªèa': 'o·∫£', '√µa': 'o√£', '·ªça': 'o·∫°', '√≤e': 'o√®', '√≥e': 'o√©','·ªèe': 'o·∫ª',\n",
    "        '√µe': 'o·∫Ω', '·ªçe': 'o·∫π', '√πy': 'u·ª≥', '√∫y': 'u√Ω', '·ªßy': 'u·ª∑', '≈©y': 'u·ªπ','·ª•y': 'u·ªµ', 'u·∫£': '·ªßa',\n",
    "        'aÃâ': '·∫£', '√¥ÃÅ': '·ªë', 'u¬¥': '·ªë','√¥ÃÉ': '·ªó', '√¥ÃÄ': '·ªì', '√¥Ãâ': '·ªï', '√¢ÃÅ': '·∫•', '√¢ÃÉ': '·∫´', '√¢Ãâ': '·∫©',\n",
    "        '√¢ÃÄ': '·∫ß', 'oÃâ': '·ªè', '√™ÃÄ': '·ªÅ','√™ÃÉ': '·ªÖ', 'ƒÉÃÅ': '·∫Ø', 'uÃâ': '·ªß', '√™ÃÅ': '·∫ø', '∆°Ãâ': '·ªü', 'iÃâ': '·ªâ',\n",
    "        'eÃâ': '·∫ª', '√†k': u' √† ','aÀã': '√†', 'iÀã': '√¨', 'ƒÉ¬¥': '·∫Ø','∆∞Ãâ': '·ª≠', 'eÀú': '·∫Ω', 'yÀú': '·ªπ', 'a¬¥': '√°',\n",
    "        \n",
    "        #Chu·∫©n h√≥a 1 s·ªë sentiment words/English words\n",
    "        ':))': '  positive ', ':)': ' positive ', '√¥ k√™i': ' ok ', 'okie': ' ok ', ' o k√™ ': ' ok ',\n",
    "        'okey': ' ok ', '√¥k√™': ' ok ', 'oki': ' ok ', ' oke ':  ' ok ',' okay':' ok ','ok√™':' ok ',\n",
    "        ' tks ': u' c√°m ∆°n ', 'thks': u' c√°m ∆°n ', 'thanks': u' c√°m ∆°n ', 'ths': u' c√°m ∆°n ', 'thank': u' c√°m ∆°n ',\n",
    "        '‚≠ê': 'star ', '*': 'star ', 'üåü': 'star ', 'üéâ': u' 5star ',\n",
    "        'kg ': u' kh√¥ng ','not': u' kh√¥ng ', u' kg ': u' kh√¥ng ', '\"k ': u' kh√¥ng ',' kh ':u' kh√¥ng ','k√¥':u' kh√¥ng ','hok':u' kh√¥ng ',' kp ': u' kh√¥ng ph·∫£i ',u' k√¥ ': u' kh√¥ng ', '\"ko ': u' kh√¥ng ', u' ko ': u' kh√¥ng ', u' k ': u' kh√¥ng ', 'khong': u' kh√¥ng ', u' hok ': u' kh√¥ng ',\n",
    "        'he he': ' 5star ','hehe': ' 5star ','hihi': ' 5star ', 'haha': ' 5star ', 'hjhj': ' 5star ',\n",
    "        ' lol ': ' 1star ',' cc ': ' 1star ','cute': u' d·ªÖ th∆∞∆°ng ','huhu': ' 1star ', ' vs ': u' v·ªõi ', 'wa': ' qu√° ', 'w√°': u' qu√°', 'j': u' g√¨ ', '‚Äú': ' ',\n",
    "        ' sz ': u' c·ª° ', 'size': u' c·ª° ', u' ƒëx ': u' ƒë∆∞·ª£c ', 'dk': u' ƒë∆∞·ª£c ', 'dc': u' ƒë∆∞·ª£c ', 'ƒëk': u' ƒë∆∞·ª£c ',\n",
    "        'ƒëc': u' ƒë∆∞·ª£c ','authentic': u' chu·∫©n ch√≠nh h√£ng ',u' aut ': u' chu·∫©n ch√≠nh h√£ng ', u' auth ': u' chu·∫©n ch√≠nh h√£ng ', 'thick': u' positive ', 'store': u' c·ª≠a h√†ng ',\n",
    "        'shop': u' c·ª≠a h√†ng ', 'sp': u' s·∫£n ph·∫©m ', 'gud': u' t·ªët ','god': u' t·ªët ','wel done':' t·ªët ', 'good': u' t·ªët ', 'g√∫t': u' t·ªët ','great': u' t·ªët ',\n",
    "        's·∫•u': u' x·∫•u ','gut': u' t·ªët ', u' tot ': u' t·ªët ', u' nice ': u' t·ªët ', 'perfect': 'r·∫•t t·ªët', 'bt': u' b√¨nh th∆∞·ªùng ',\n",
    "        'time': u' th·ªùi gian ', 'q√°': u' qu√° ', u' ship ': u' giao h√†ng ', u' m ': u' m√¨nh ', u' mik ': u' m√¨nh ',\n",
    "        '√™Ãâ': '·ªÉ', 'product': 's·∫£n ph·∫©m', 'quality': 'ch·∫•t l∆∞·ª£ng','chat':' ch·∫•t ', 'excelent': 'ho√†n h·∫£o', 'bad': 't·ªá','fresh': ' t∆∞∆°i ','sad': ' t·ªá ',\n",
    "        'date': u' h·∫°n s·ª≠ d·ª•ng ', 'hsd': u' h·∫°n s·ª≠ d·ª•ng ','quickly': u' nhanh ', 'quick': u' nhanh ','fast': u' nhanh ','delivery': u' giao h√†ng ',u' s√≠p ': u' giao h√†ng ',\n",
    "        'beautiful': u' ƒë·∫πp tuy·ªát v·ªùi ', u' tl ': u' tr·∫£ l·ªùi ', u' r ': u' r·ªìi ', u' shopE ': u' c·ª≠a h√†ng ',u' order ': u' ƒë·∫∑t h√†ng ',\n",
    "        'ch·∫•t lg': u' ch·∫•t l∆∞·ª£ng ',u' sd ': u' s·ª≠ d·ª•ng ',u' dt ': u' ƒëi·ªán tho·∫°i ',u' nt ': u' nh·∫Øn tin ',u' tl ': u' tr·∫£ l·ªùi ',u' s√†i ': u' x√†i ',u'bjo':u' bao gi·ªù ',\n",
    "        'thik': u' th√≠ch ',u' sop ': u' c·ª≠a h√†ng ', ' fb ': ' facebook ', ' face ': ' facebook ', ' very ': u' r·∫•t ',u'qu·∫£ ng ':u' qu·∫£ng  ',\n",
    "        'dep': u' ƒë·∫πp ',u' xau ': u' x·∫•u ','delicious': u' ngon ', u'h√†g': u' h√†ng ', u'q·ªßa': u' qu·∫£ ',\n",
    "        'iu': u' y√™u ','fake': u' gi·∫£ m·∫°o ', 'trl': 'tr·∫£ l·ªùi', '><': u' 5star ',\n",
    "        ' por ': u' t·ªá ',' poor ': u' t·ªá ', 'ib':u' nh·∫Øn tin ', 'rep':u' tr·∫£ l·ªùi ',u'fback':' feedback ','fedback':' feedback ',\n",
    "        # quy c√°c icon v·ªÅ 2 lo·∫°i 1 sao v√† 5 sao:\n",
    "        \"üëπ\": \"1star\", \"üëª\": \"5star\", \"üíÉ\": \"5star\",'ü§ô': ' 5star ', 'üëç': ' 5star ',\n",
    "        \"üíÑ\": \"5star\", \"üíé\": \"5star\", \"üí©\": \"5star\",\"üòï\": \"1star\", \"üò±\": \"1star\", \"üò∏\": \"5star\",\n",
    "        \"üòæ\": \"1star\", \"üö´\": \"1star\",  \"ü§¨\": \"1star\",\"üßö\": \"5star\", \"üß°\": \"5star\",'üê∂':' 5star ',\n",
    "        'üëé': ' 1star ', 'üò£': ' 1star ','‚ú®': ' 5star ', '‚ù£': ' 5star ','‚òÄ': ' 5star ',\n",
    "        '‚ô•': ' 5star ', 'ü§©': ' 5star ', 'like': ' 5star ', 'üíå': ' 5star ',\n",
    "        'ü§£': ' 5star ', 'üñ§': ' 5star ', 'ü§§': ' 5star ', ':(': ' 1star ', 'üò¢': ' 1star ',\n",
    "        '‚ù§': ' 5star ', 'üòç': ' 5star ', 'üòò': ' 5star ', 'üò™': ' 1star ', 'üòä': ' 5star ',\n",
    "        '?': ' ? ', 'üòÅ': ' 5star ', 'üíñ': ' 5star ', 'üòü': ' 1star ', 'üò≠': ' 1star ',\n",
    "        'üíØ': ' 5star ', 'üíó': ' 5star ', '‚ô°': ' 5star ', 'üíú': ' 5star ', 'ü§ó': ' 5star ',\n",
    "        '^^': ' 5star ', 'üò®': ' 1star ', '‚ò∫': ' 5star ', 'üíã': ' 5star ', 'üëå': ' 5star ',\n",
    "        'üòñ': ' 1star ', 'üòÄ': ' 5star ', ':((': ' 1star ', 'üò°': ' 1star ', 'üò†': ' 1star ',\n",
    "        'üòí': ' 1star ', 'üôÇ': ' 5star ', 'üòè': ' 1star ', 'üòù': ' 5star ', 'üòÑ': ' 5star ',\n",
    "        'üòô': ' 5star ', 'üò§': ' 1star ', 'üòé': ' 5star ', 'üòÜ': ' 5star ', 'üíö': ' 5star ',\n",
    "        '‚úå': ' 5star ', 'üíï': ' 5star ', 'üòû': ' 1star ', 'üòì': ' 1star ', 'Ô∏èüÜóÔ∏è': ' 5star ',\n",
    "        'üòâ': ' 5star ', 'üòÇ': ' 5star ', ':v': '  5star ', '=))': '  5star ', 'üòã': ' 5star ',\n",
    "        'üíì': ' 5star ', 'üòê': ' 1star ', ':3': ' 5star ', 'üò´': ' 1star ', 'üò•': ' 1star ',\n",
    "        'üòÉ': ' 5star ', 'üò¨': ' üò¨ ', 'üòå': ' üòå ', 'üíõ': ' 5star ', 'ü§ù': ' 5star ', 'üéà': ' 5star ',\n",
    "        'üòó': ' 5star ', 'ü§î': ' 1star ', 'üòë': ' 1star ', 'üî•': ' 1star ', 'üôè': ' 1star ',\n",
    "        'üÜó': ' 5star ', 'üòª': ' 5star ', 'üíô': ' 5star ', 'üíü': ' 5star ',\n",
    "        'üòö': ' 5star ', '‚ùå': ' 1star ', 'üëè': ' 5star ', ';)': ' 5star ', '<3': ' 5star ',\n",
    "        'üåù': ' 5star ',  'üå∑': ' 5star ', 'üå∏': ' 5star ', 'üå∫': ' 5star ',\n",
    "        'üåº': ' 5star ', 'üçì': ' 5star ', 'üêÖ': ' 5star ', 'üêæ': ' 5star ', 'üëâ': ' 5star ',\n",
    "        'üíê': ' 5star ', 'üíû': ' 5star ', 'üí•': ' 5star ', 'üí™': ' 5star ',\n",
    "        'üí∞': ' 5star ',  'üòá': ' 5star ', 'üòõ': ' 5star ', 'üòú': ' 5star ',\n",
    "        'üôÉ': ' 5star ', 'ü§ë': ' 5star ', 'ü§™': ' 5star ','‚òπ': ' 1star ',  'üíÄ': ' 1star ',\n",
    "        'üòî': ' 1star ', 'üòß': ' 1star ', 'üò©': ' 1star ', 'üò∞': ' 1star ', 'üò≥': ' 1star ',\n",
    "        'üòµ': ' 1star ', 'üò∂': ' 1star ', 'üôÅ': ' 1star ',\n",
    "        #\n",
    "        'nƒÉm sao': ' 5star ','5 sao': ' 5star ', '4 sao': ' 4star ','b·ªën sao': ' 4star ','3 sao': ' 3star ',\n",
    "        'ba sao': ' 3star ', '2 sao': ' 2star ', 'hai sao': ' 2star ','1 sao':' 1star ','m·ªôt sao':' 1star ',\n",
    "        }\n",
    "\n",
    "    for k, v in replace_list.items():\n",
    "        text = text.replace(k, v)\n",
    "\n",
    "    # chuyen punctuation th√†nh space\n",
    "    translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "    text = text.translate(translator)\n",
    "\n",
    "    text = ViTokenizer.tokenize(text)\n",
    "    texts = text.split()\n",
    "    len_text = len(texts)\n",
    "\n",
    "    texts = [t.replace('_', ' ') for t in texts]\n",
    "    \n",
    "\n",
    "    text = u' '.join(texts)\n",
    "\n",
    "    #remove n·ªët nh·ªØng k√Ω t·ª± th·ª´a th√£i\n",
    "    text = text.replace(u'\"', u' ')\n",
    "    text = text.replace(u'Ô∏è', u'')\n",
    "    text = text.replace('üèª','')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSource(object):\n",
    "  def load_data(self, filename):\n",
    "    star= []\n",
    "    review = []\n",
    "\n",
    "    with open(filename, 'r') as fp:\n",
    "        line = fp.readline()\n",
    "        while line:\n",
    "            star.append(line[0])\n",
    "            review.append(line[2:])\n",
    "            line = fp.readline()\n",
    "    \n",
    "    return star, review\n",
    "    \n",
    "  def load_test_data(self, filename):\n",
    "    star= []\n",
    "    review = []\n",
    "\n",
    "    with open(filename, 'r') as fp:\n",
    "        line = fp.readline()\n",
    "        while line:\n",
    "            star.append(line[1])\n",
    "            review.append(line[3:-1])\n",
    "            line = fp.readline()\n",
    "    \n",
    "    return star, review\n",
    "  def load_data_2(self, filename):\n",
    "    star= []\n",
    "    review = []\n",
    "    with open(filename, 'r') as csv_file:\n",
    "      i = 0\n",
    "      for row in csv_file:\n",
    "          row = row.split('<fff>')\n",
    "          star.append(row[0])\n",
    "          review.append(row[1])\n",
    "    return star, review\n",
    "  def transform_to_dataset(self, x_set, y_set):\n",
    "    X, y = [], []\n",
    "    for document, topic in zip(list(x_set), list(y_set)):\n",
    "        document = normalize_text(document)\n",
    "        X.append(document.strip())\n",
    "        y.append(topic)\n",
    "    return X, y\n",
    "  def transform_to_dataset_with_augmentation(self, x_set, y_set):\n",
    "    X, y = [], []\n",
    "    for document, topic in zip(list(x_set), list(y_set)):\n",
    "        document = normalize_text(document)\n",
    "        X.append(document.strip())\n",
    "        y.append(topic)\n",
    "        # Augmentation b·∫±ng c√°ch remove d·∫•u ti·∫øng Vi·ªát\n",
    "        X.append(no_marks(document))\n",
    "        y.append(topic)\n",
    "    return X, y\n",
    "  def get_predicted_result(self, star, review, filename):\n",
    "    f = open(filename, \"a\")\n",
    "    for i in range(len(star)):\n",
    "      line = str(star[i]) + \" \" + review[i] + \"\\n\"\n",
    "      f.write(line)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DataSource()\n",
    "star, review = ds.load_data('train.txt')\n",
    "star2, review2 = ds.load_data_2('bag_text.txt')\n",
    "star_test, review_test = ds.load_test_data('test.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  X_train, X_test, y_train, y_test = train_test_split(review, star, test_size=0.6, random_state=42)\n",
    "\n",
    "X_train = review\n",
    "y_train = star\n",
    "X_train = X_train * 50 +  review2 * 50 \n",
    "y_train = y_train * 50 +  star2 * 50\n",
    "X_train, y_train = ds.transform_to_dataset_with_augmentation(X_train,y_train)\n",
    "X_test, y_test = ds.transform_to_dataset(review_test, star_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_ws = (u'r·∫±ng',u'th√¨',u'l√†')\n",
    "#Try some models\n",
    "classifiers = [\n",
    "            MultinomialNB(),\n",
    "            # DecisionTreeClassifier(),\n",
    "            # LogisticRegression(),\n",
    "            SGDClassifier(),\n",
    "            LinearSVC(fit_intercept = True,multi_class='crammer_singer', C=1),\n",
    "            # RandomForestClassifier(),\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'5': 450, '4': 99, '3': 31, '1': 19, '2': 1})\n",
      "Counter({'5': 400, '4': 114, '3': 61, '1': 24, '2': 1})\n",
      "Counter({'5': 412, '4': 108, '3': 55, '1': 23, '2': 2})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hatake/.local/lib/python3.6/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,4))\n",
    "vectors = vectorizer.fit_transform(X_train)\n",
    "vectors1 = vectorizer.transform(X_test)\n",
    "i = 1\n",
    "for classifier in classifiers:\n",
    "    filename = \"result_\" + str(i) + \".txt\"\n",
    "#     steps = []\n",
    "#     steps.append(('CountVectorizer', CountVectorizer(ngram_range=(1,3),stop_words=stop_ws)))\n",
    "#     steps.append(('tfidf', TfidfTransformer(use_idf=False, sublinear_tf = True,norm='l2',smooth_idf=True)))\n",
    "#     steps.append(('classifier', classifier))\n",
    "#     clf = Pipeline(steps)\n",
    "#     clf.fit(X_train, y_train)\n",
    "    \n",
    "    classifier.fit(vectors, y_train)\n",
    "    y_pred = classifier.predict(vectors1)\n",
    "    print(Counter(y_pred))\n",
    "    ds.get_predicted_result(y_pred, review_test, filename)\n",
    "    i = i+1\n",
    "#     score = accuracy_score(y_test, y_pred)\n",
    "#     print(classifier)\n",
    "#     print(score)\n",
    "#     print(confusion_matrix(y_test, y_pred))\n",
    "#     print(cross_score = cross_val_score(classifier, X_train,y_train, cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
